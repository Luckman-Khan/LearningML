{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6357fc64-0233-43a5-a32c-24bf54b3e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccdd23ec-ff31-4c58-a30f-c562723b9337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\english_wordnet.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package mock_corpus to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Luckman\n",
      "[nltk_data]    |     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa4374c-76e8-47da-9188-6fe51d2f20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt=\"Hello Geeks. We're hoping you guys are doing great.\"\n",
    "print(len(txt.split('.')))#this gives number of sentences\n",
    "print(len(txt.split()))#this gives number of words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f06991-4b59-4ecf-952b-d5751c838ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,sent_tokenize\n",
    "print(len(word_tokenize(txt)))\n",
    "print(len(sent_tokenize(txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9342829-3d27-46bf-890e-e252720e18b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Geeks\n",
      ".\n",
      "We\n",
      "'re\n",
      "hoping\n",
      "you\n",
      "guys\n",
      "are\n",
      "doing\n",
      "great\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for words in word_tokenize(txt):\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1461df9a-1967-413c-bb63-97b66d444510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Geeks\n",
      "We\n",
      "'re\n",
      "hoping\n",
      "you\n",
      "guys\n",
      "are\n",
      "doing\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "for words in word_tokenize(txt):\n",
    "    if words != '.':\n",
    "     print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc99586-8573-4403-9bf9-653c547155c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Luckman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Luckman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6761974b-0ba2-45ba-b095-10fe893dbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer,PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1be6380-eb66-4221-82b1-12ec44345e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "change\n",
      "changed\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize('change'))\n",
    "print(lem.lemmatize('changes'))\n",
    "print(lem.lemmatize('changed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cfe8f2-5a87-438a-8a59-919c0cb389fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chang\n",
      "chang\n",
      "chang\n"
     ]
    }
   ],
   "source": [
    "print(stem.stem('change'))\n",
    "print(stem.stem('changes'))\n",
    "print(stem.stem('changed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8543285e-a0c8-4dbf-b8dd-3ef734da4d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Luckman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cf45ff-2f33-47ee-ac13-ad14b469c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#To check all the English stop words\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cb2e019-6d10-421f-a889-a0199336418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "not\n",
      "a\n",
      "good\n",
      "time\n",
      "to\n",
      "talk\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "txt = \"This is not a good time to talk.\"\n",
    "txt = word_tokenize(txt)\n",
    "for word in txt:\n",
    "    if word != '.':\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da221c7-299d-4fb0-a142-7fa83e6ef27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "time\n",
      "talk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "txt = \"This is not the right time to talk.\"\n",
    "txt = word_tokenize(txt)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [w for w in txt if w.lower() not in stop_words]\n",
    "for word in filtered_words:\n",
    "    if word != '.':\n",
    "        print(word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1d0ef5-e14f-4415-8219-082ed17ba2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'T': 84}, {'h': 104}, {'e': 101}, {' ': 32}, {'q': 113}, {'u': 117}, {'i': 105}, {'c': 99}, {'k': 107}, {' ': 32}, {'b': 98}, {'r': 114}, {'o': 111}, {'w': 119}, {'n': 110}, {' ': 32}, {'f': 102}, {'o': 111}, {'x': 120}, {' ': 32}, {'j': 106}, {'u': 117}, {'m': 109}, {'p': 112}, {'s': 115}, {' ': 32}, {'o': 111}, {'v': 118}, {'e': 101}, {'r': 114}, {' ': 32}, {'t': 116}, {'h': 104}, {'e': 101}, {' ': 32}, {'l': 108}, {'a': 97}, {'z': 122}, {'y': 121}, {' ': 32}, {'d': 100}, {'o': 111}, {'g': 103}]\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "print([{char:ord(char)}for char in text ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f2d991-11c7-4605-8a8e-78bc9b756318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character value of 56:D\n"
     ]
    }
   ],
   "source": [
    "print(f\"The character value of 56:{chr(68)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5c5325-6e99-4c1d-8e9c-1d78fc323fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character value of 97 is  a\n",
      "The character value of 65 is  A\n",
      "The character value of 93 is  ]\n"
     ]
    }
   ],
   "source": [
    "#To print the character value for 97\n",
    "print(\"The character value of 97 is \",chr(97))\n",
    "#To print the character value of 65\n",
    "print(\"The character value of 65 is \",chr(65))\n",
    "#To print the character value of 93\n",
    "print(\"The character value of 93 is \",chr(93))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cae773a-69d3-4146-9dea-2608c79e5040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ascii_ = [ord(char) for char in text]\n",
    "''.join([chr(asc) for asc in ascii_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce2f8dd-58e3-40e3-8449-af5405ce41d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([chr(asc) for asc in range(65,123)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987e9ec7-2f5c-47c1-b44c-985f7e5829f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppr(text):\n",
    "    string=''\n",
    "    for char in text:\n",
    "        if ord(char)>=97 and ord(char)<=122:\n",
    "            string+= chr(ord(char)-32)\n",
    "        else:\n",
    "            string+=char\n",
    "    return string\n",
    "def lowr(text):\n",
    "    string = ''\n",
    "    for char in text:\n",
    "        if ord(char) >=65 and ord(char)<=90:\n",
    "            string+=chr(ord(char)+32)\n",
    "        else:\n",
    "            string+=char\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d37d13-66e8-4822-9e77-89c2c974ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELHI\n",
      "kolkata\n"
     ]
    }
   ],
   "source": [
    "print(uppr('delhi'))\n",
    "print(lowr('kOlKaTA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42092b20-8a55-4c62-8d0a-190fd92fef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def check_alpha(text):\n",
    "    c=0\n",
    "    for i in text:\n",
    "        if (ord(i)>= 65 and ord(i)<=90) or (ord(i)>=97 and ord(i)<=122):\n",
    "            c+=1\n",
    "    if (len(text) == c):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "print(check_alpha(\"howdy\"))\n",
    "print(check_alpha(\"howdy!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0910af-5481-4291-974c-aed882e4bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def check_digit(text):\n",
    "    c=0\n",
    "    for i in text:\n",
    "        if((ord(i)>= 48 and ord(i)<=57)):\n",
    "            c+=1\n",
    "    if (len(text) == c):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "print(check_digit(\"1234\"))\n",
    "print(check_digit(\"1f3g4tt!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459e3f1e-da4b-45d9-af78-aaccaa37e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Brown Fox Jump Over The Laxy Dog\n"
     ]
    }
   ],
   "source": [
    "def Title(text):\n",
    "    output = []\n",
    "\n",
    "    for word in text.split(' '):\n",
    "        output.append(uppr(word[0]) + lowr(word[1:]))\n",
    "    return ' '.join(output)\n",
    "print(Title(\"the brown fox jump over the laxy dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21683619-c2ae-40c1-94c7-382fb6bb0441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='hello'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.search('hello','hello world'))\n",
    "print(re.search('data','hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6fdb07d-9e52-4dd1-a393-4c09fd66cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5)\n",
      "0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "s = re.search('hello','hello world')\n",
    "print(s.span())\n",
    "print(s.start())\n",
    "print(s.end())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c30c8b77-e018-4276-a50d-97fc5a18446f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'are', 'are']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= \"hello how are area are you ?\"\n",
    "b=\"are\"\n",
    "re.findall(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc4788f7-ccad-4cec-a9f0-e46083574d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5)\n",
      "(12, 17)\n",
      "(25, 30)\n"
     ]
    }
   ],
   "source": [
    "for i in re.finditer('hello','hello world hello galaxy hello universe'):\n",
    "    print(i.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b09828d-9078-47d2-886b-e58819368876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(23, 35), match='834-4323-345'>\n",
      "None\n",
      "834-4323-345\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"my telephone number is 834-4323-345\"\n",
    "txt2 = \"my telephone number is 8344323345\"\n",
    "pattern = '\\d{3}-\\d{4}-\\d{3}'\n",
    "print(re.search(pattern,txt1))\n",
    "print(re.search(pattern,txt2))\n",
    "print(re.search(pattern,txt1).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2feb4143-cfd3-4eef-bc16-84a678efaa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sat', 'mat', ' at', 'rat']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('.at','The cat sat on the mat and attacked the rat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14967264-87e5-45a3-adf5-27ccf67ba322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('\\d','4 is divisible by 2 and not by 3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7d61a73-bf08-4836-aff4-478a3be3270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4']\n",
      "[]\n",
      "['3']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('^\\d','4 is divisible bt 2 and not by 3'))\n",
    "print(re.findall('^\\d','Hi, 4 is divisible by 2 and not by 3'))\n",
    "print(re.findall('\\d$','4 is divisible by 2 and not by 3'))\n",
    "print(re.findall('\\d$','4 is divisible by 2 and not by 3 in maths'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "259a8a67-10c0-471e-b172-497b5f10378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'G', 'F', 'G']\n",
      "['e', 'l', 'c', 'o', 'm', 'e', 't', 'o']\n",
      "['1']\n",
      "['W', 'e', 'l', 'c', 'o', 'm', 'e', 't', 'o', 'G', 'F', 'G']\n",
      "['W', 'e', 'l', 'c', 'o', 'm', 'e', 't', 'o', 'G', 'F', 'G', '1']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('[A-Z]','Welcome to GFG 1'))\n",
    "print(re.findall('[a-z]','Welcome to GFG 1'))\n",
    "print(re.findall('[0-9]','Welcome to GFG 1'))\n",
    "print(re.findall('[A-Za-z]','Welcome to GFG 1'))\n",
    "print(re.findall('[A-Za-z0-9]','Welcome to GFG 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9adbf3d-5065-4d9c-9471-95769b1a618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eshant ', ' is happ', 'y ']\n"
     ]
    }
   ],
   "source": [
    "txt=\"Eshant $ is happ@y !,\"\n",
    "print(re.findall('[^!,$@]+',txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0241fe91-5762-4309-914b-1758f70b9598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eshant  is happy \n"
     ]
    }
   ],
   "source": [
    "print(''.join(re.findall('[^!,@#$%^&*]+',txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40ba1479-09c7-40c4-8d43-1b41954e792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eshant ', ' is happ', 'y ']\n"
     ]
    }
   ],
   "source": [
    "print((re.findall('[^!,@#$%^&*]+',txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03d068d0-b2d0-46bf-a4fb-c23843dbf499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 's', 'h', 'a', 'n', 't', ' ', ' ', 'i', 's', ' ', 'h', 'a', 'p', 'p', 'y', ' ']\n"
     ]
    }
   ],
   "source": [
    "print((re.findall('[^!,@#$%^&*]',txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e862a585-8d0a-4e91-a1c5-ef6e675776bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hii I am Luan and \n"
     ]
    }
   ],
   "source": [
    "txt = \"Hii I am Luan and 20\"\n",
    "print(''.join(re.findall('\\D',txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a31a2c7-2d1f-4440-adae-9e2faceea4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geeks-for-Geeks', 'works-it-out', 'wklfd-dfjgk-fjkds']\n"
     ]
    }
   ],
   "source": [
    "txt=\"Hello I am studying from Geeks-for-Geeks and it is amazing.Let's see how this works-it-out, wklfd-dfjgk-fjkds\"\n",
    "print(re.findall('[\\w]+-[\\w]+-[\\w]+',txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1cbaa1c-db03-4f8f-80b3-ba6e6f6ae020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['64-6534-3']\n",
      "['543-5345-6']\n",
      "['4563-453-4']\n",
      "['53-5453-5']\n",
      "['435-234-6']\n"
     ]
    }
   ],
   "source": [
    "#square brackets are redundant and what matters is + sign which states there can be one or more numbers\n",
    "for no in [\"64-6534-342\", \"543-5345-645\",\"4563-453-445\",\"53-5453-5345\",\"435-234-6324\"]:\n",
    "    print(str(re.findall('[\\d]+-[\\d]+-[\\d]' , no)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53232efd-51da-4cb2-a41c-f161bbd89655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6465343\n",
      "54353456\n",
      "45634534\n",
      "5354535\n",
      "4352346\n"
     ]
    }
   ],
   "source": [
    "for no in [\"64-6534-342\", \"543-5345-645\",\"4563-453-445\",\"53-5453-5345\",\"435-234-6324\"]:\n",
    "    \n",
    "    print(str(re.findall('[\\d]+-[\\d]+-[\\d]' , no)[0]).replace('-',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dbc3a6d0-26da-42d2-b50d-7e2a00692570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 17), match='ehafhai@gmail.com'>\n"
     ]
    }
   ],
   "source": [
    "txt='Mail eshant@gfg.in to contact Eshant'\n",
    "p = '[A-Za-z0-9]+@[\\w]+.[\\w]+'\n",
    "print(re.search(p,mail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24f07491-d05e-4613-be1a-0efa4824fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 14), match='eshant@gfg.org'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mail1='eshant@gfg.org'\n",
    "mail2='eshant@gmail.com'\n",
    "p='[A-Za-z0-2]+@(gfg).(org)'\n",
    "print(re.search(p,mail1))\n",
    "print(re.search(p,mail2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9d5a17a-f1ef-4f2f-8b1a-63e3344cd2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 14), match='eshant@gfg.org'>\n",
      "<re.Match object; span=(0, 14), match='eshant@gfg.net'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mail1='eshant@gfg.org'\n",
    "mail2='eshant@gfg.net'\n",
    "mail3='eshant@gmail.com'\n",
    "\n",
    "p = '[A-Za-z0-9]+@(gfg).(org|net|in)'\n",
    "\n",
    "print(re.search(p,mail1))\n",
    "print(re.search(p,mail2))\n",
    "print(re.search(p,mail3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ba8bb62-33f5-4db1-a3a3-d54ea7c00fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8372493\n",
      "423423\n"
     ]
    }
   ],
   "source": [
    "txt = 'The number is 8372493 not 423423'\n",
    "c = 0  \n",
    "for i in range(len(txt)):\n",
    " \n",
    "    if txt[i].isdigit() == True:\n",
    "      \n",
    "        if c == 0:\n",
    "            st = i \n",
    "        c += 1 \n",
    "    else:\n",
    "       \n",
    "        if c != 0:\n",
    "            end = i\n",
    "            print(txt[st:end])\n",
    "            c = 0  \n",
    "\n",
    "if c != 0:\n",
    "   \n",
    "    print(txt[-c:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "273302e0-2ec5-420d-93fd-4a3c64316c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 17\n"
     ]
    }
   ],
   "source": [
    "txt=\"Let us study Data Science from GFG.\"\n",
    "txt = txt.lower()\n",
    "word = \"data\"\n",
    "\n",
    "for i in range(len(txt)):\n",
    "    if(txt[i:i+len(word)]==word):\n",
    "        print(i,i+len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7ac58bb-a854-4eea-b531-b1f3eb17d3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industries.\n",
      "Industry\n"
     ]
    }
   ],
   "source": [
    "txt = 'AI is having the capability to revolutionize all the industries. The Industry is ready for a big change'\n",
    "word = 'in'\n",
    "\n",
    "for wrd in txt.split(' '):\n",
    "    if (wrd[:2].lower() == word.lower()):\n",
    "        print(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3995562b-7735-4e25-aa50-91789124e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "the\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "txt = 'AI is having the capability to revolutionize all the industries. The Industry is ready for a big change'\n",
    "word = 'he'\n",
    "\n",
    "for wrd in txt.split():\n",
    "    if wrd[-2:].lower() == word.lower():\n",
    "        print(wrd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d62715-33ca-483c-b4c0-f0156a5b263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is\n",
      "is having\n",
      "having the\n",
      "the capability\n",
      "capability to\n",
      "to revolutionize\n",
      "revolutionize all\n",
      "all the\n",
      "the industries.\n",
      "industries. The\n",
      "The Industry\n",
      "Industry is\n",
      "is ready\n",
      "ready for\n",
      "for a\n",
      "a big\n",
      "big change\n"
     ]
    }
   ],
   "source": [
    "txt = 'AI is having the capability to revolutionize all the industries. The Industry is ready for a big change'\n",
    "for i in range(len(txt.split(' '))-1):\n",
    "    print(txt.split(' ')[i],txt.split(' ')[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c34335d8-1dd8-4cc4-967b-586071c3a87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eshant', 'eshant', 'eshant', 'eshant', 'esh', 'esh', 'eshant']\n",
      "['gmail', 'gfg', 'yahoo', 'aap', 'geeksforgeeks', 'mail', 'orkut']\n",
      "['com', 'org', 'com', 'gov.in', 'com', 'com', 'com']\n"
     ]
    }
   ],
   "source": [
    "mails = ['eshant@gmail.com','eshant@gfg.org','eshant@yahoo.com','eshant@aap.gov.in','esh@geeksforgeeks.com','esh@mail.com','eshant@orkut.com']\n",
    "\n",
    "user_id = []\n",
    "host_name = []\n",
    "domain_name = []\n",
    "\n",
    "for mail in mails:\n",
    "    user_id.append(mail.split('@')[0])\n",
    "    host_name.append(mail.split('@')[1].split('.')[0])\n",
    "    domain_name.append('.'.join(mail.split('@')[1].split('.')[1:]))\n",
    "print(user_id)\n",
    "print(host_name)\n",
    "print(domain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d060e355-39fa-4b86-94e4-ce9563b8105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\luckman khan\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 3.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.3/13.9 MB 3.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.6/13.9 MB 4.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.4/13.9 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.2/13.9 MB 4.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.0/13.9 MB 4.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.8/13.9 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.8/13.9 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.1/13.9 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.3/13.9 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.3/13.9 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.6/13.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.9/13.9 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.4/13.9 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.0/13.9 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.5/13.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
      "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 630.6/630.6 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 4.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.0/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.1/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl (139 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "\n",
      "   ---- -----------------------------------  2/18 [spacy-loggers]\n",
      "   -------- -------------------------------  4/18 [smart-open]\n",
      "   --------------- ------------------------  7/18 [cloudpathlib]\n",
      "   -------------------- -------------------  9/18 [blis]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ------------------------ --------------- 11/18 [preshed]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   ---------------------------- ----------- 13/18 [langcodes]\n",
      "   --------------------------------- ------ 15/18 [weasel]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ---------------------------------------- 18/18 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 preshed-3.0.10 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202048aa-299d-40f1-8f30-7978c8d5bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.9/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 4.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.5/33.5 MB 3.1 MB/s eta 0:00:11\n",
      "      --------------------------------------- 0.8/33.5 MB 1.7 MB/s eta 0:00:20\n",
      "     - -------------------------------------- 1.6/33.5 MB 2.3 MB/s eta 0:00:14\n",
      "     -- ------------------------------------- 2.1/33.5 MB 2.5 MB/s eta 0:00:13\n",
      "     --- ------------------------------------ 2.9/33.5 MB 2.6 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 3.1/33.5 MB 2.6 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 3.7/33.5 MB 2.4 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 4.2/33.5 MB 2.4 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 4.5/33.5 MB 2.5 MB/s eta 0:00:12\n",
      "     ------ --------------------------------- 5.2/33.5 MB 2.4 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 6.0/33.5 MB 2.5 MB/s eta 0:00:11\n",
      "     -------- ------------------------------- 6.8/33.5 MB 2.7 MB/s eta 0:00:11\n",
      "     -------- ------------------------------- 7.3/33.5 MB 2.7 MB/s eta 0:00:10\n",
      "     --------- ------------------------------ 7.6/33.5 MB 2.7 MB/s eta 0:00:10\n",
      "     --------- ------------------------------ 7.6/33.5 MB 2.7 MB/s eta 0:00:10\n",
      "     --------- ------------------------------ 8.1/33.5 MB 2.4 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 8.7/33.5 MB 2.4 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 8.7/33.5 MB 2.4 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 9.2/33.5 MB 2.2 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 9.4/33.5 MB 2.3 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 9.7/33.5 MB 2.2 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 10.0/33.5 MB 2.1 MB/s eta 0:00:12\n",
      "     ------------ --------------------------- 10.5/33.5 MB 2.1 MB/s eta 0:00:11\n",
      "     ------------ --------------------------- 10.7/33.5 MB 2.1 MB/s eta 0:00:11\n",
      "     ------------ --------------------------- 10.7/33.5 MB 2.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 11.0/33.5 MB 2.0 MB/s eta 0:00:12\n",
      "     ------------- -------------------------- 11.0/33.5 MB 2.0 MB/s eta 0:00:12\n",
      "     ------------- -------------------------- 11.3/33.5 MB 1.9 MB/s eta 0:00:12\n",
      "     ------------- -------------------------- 11.5/33.5 MB 1.9 MB/s eta 0:00:12\n",
      "     -------------- ------------------------- 11.8/33.5 MB 1.8 MB/s eta 0:00:12\n",
      "     -------------- ------------------------- 12.1/33.5 MB 1.8 MB/s eta 0:00:12\n",
      "     -------------- ------------------------- 12.1/33.5 MB 1.8 MB/s eta 0:00:12\n",
      "     -------------- ------------------------- 12.3/33.5 MB 1.8 MB/s eta 0:00:13\n",
      "     --------------- ------------------------ 12.6/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     --------------- ------------------------ 12.8/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     --------------- ------------------------ 13.1/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     --------------- ------------------------ 13.4/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------- ----------------------- 13.6/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------- ----------------------- 13.6/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     ---------------- ----------------------- 13.9/33.5 MB 1.7 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 14.4/33.5 MB 1.6 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 14.7/33.5 MB 1.6 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 14.9/33.5 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------ --------------------- 15.2/33.5 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------ --------------------- 15.5/33.5 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------ --------------------- 15.7/33.5 MB 1.6 MB/s eta 0:00:12\n",
      "     ------------------- -------------------- 16.0/33.5 MB 1.6 MB/s eta 0:00:11\n",
      "     ------------------- -------------------- 16.3/33.5 MB 1.6 MB/s eta 0:00:11\n",
      "     ------------------- -------------------- 16.5/33.5 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------- ------------------- 17.0/33.5 MB 1.6 MB/s eta 0:00:11\n",
      "     -------------------- ------------------- 17.6/33.5 MB 1.6 MB/s eta 0:00:10\n",
      "     --------------------- ------------------ 18.1/33.5 MB 1.6 MB/s eta 0:00:10\n",
      "     ---------------------- ----------------- 18.6/33.5 MB 1.6 MB/s eta 0:00:10\n",
      "     ---------------------- ----------------- 18.9/33.5 MB 1.6 MB/s eta 0:00:09\n",
      "     ---------------------- ----------------- 19.1/33.5 MB 1.6 MB/s eta 0:00:09\n",
      "     ----------------------- ---------------- 19.7/33.5 MB 1.6 MB/s eta 0:00:09\n",
      "     ----------------------- ---------------- 19.9/33.5 MB 1.6 MB/s eta 0:00:09\n",
      "     ------------------------ --------------- 20.2/33.5 MB 1.6 MB/s eta 0:00:09\n",
      "     ------------------------ --------------- 20.7/33.5 MB 1.7 MB/s eta 0:00:08\n",
      "     ------------------------- -------------- 21.2/33.5 MB 1.7 MB/s eta 0:00:08\n",
      "     ------------------------- -------------- 21.8/33.5 MB 1.7 MB/s eta 0:00:08\n",
      "     -------------------------- ------------- 22.3/33.5 MB 1.7 MB/s eta 0:00:07\n",
      "     -------------------------- ------------- 22.5/33.5 MB 1.7 MB/s eta 0:00:07\n",
      "     --------------------------- ------------ 23.1/33.5 MB 1.7 MB/s eta 0:00:07\n",
      "     --------------------------- ------------ 23.1/33.5 MB 1.7 MB/s eta 0:00:07\n",
      "     ---------------------------- ----------- 23.6/33.5 MB 1.7 MB/s eta 0:00:06\n",
      "     ---------------------------- ----------- 23.9/33.5 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------------------------- ---------- 24.4/33.5 MB 1.7 MB/s eta 0:00:06\n",
      "     ----------------------------- ---------- 24.6/33.5 MB 1.7 MB/s eta 0:00:06\n",
      "     ------------------------------ --------- 25.2/33.5 MB 1.7 MB/s eta 0:00:05\n",
      "     ------------------------------ --------- 25.2/33.5 MB 1.7 MB/s eta 0:00:05\n",
      "     ------------------------------ --------- 25.7/33.5 MB 1.7 MB/s eta 0:00:05\n",
      "     ------------------------------- -------- 26.2/33.5 MB 1.7 MB/s eta 0:00:05\n",
      "     ------------------------------- -------- 26.7/33.5 MB 1.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 27.0/33.5 MB 1.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 27.3/33.5 MB 1.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 27.3/33.5 MB 1.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 27.5/33.5 MB 1.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 27.5/33.5 MB 1.7 MB/s eta 0:00:04\n",
      "     --------------------------------- ------ 27.8/33.5 MB 1.6 MB/s eta 0:00:04\n",
      "     --------------------------------- ------ 28.3/33.5 MB 1.6 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 28.6/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 28.8/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 29.1/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 29.4/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 29.6/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 29.9/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 30.1/33.5 MB 1.6 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 30.4/33.5 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 30.7/33.5 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 30.9/33.5 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 31.5/33.5 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 31.7/33.5 MB 1.6 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 32.0/33.5 MB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 32.5/33.5 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  32.8/33.5 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.5/33.5 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20a2c2b9-5bc8-4670-a8ff-afdb390e1989",
   "metadata": {},
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcdc07f-3dde-4cd3-8aa8-1bfae6784a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFG\n",
      "is\n",
      "looking\n",
      "for\n",
      "data\n",
      "science\n",
      "interns\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "for token in s:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6976be77-2973-4a83-ac7d-2ad3ea620105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "cost\n",
      "of\n",
      "Iphone\n",
      "in\n",
      "U.K\n",
      "is\n",
      "699\n",
      "$\n"
     ]
    }
   ],
   "source": [
    "s = nlp('The cost of Iphone in U.K is 699$')\n",
    "for token in s:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d52de8a-6596-4dad-ad4e-efc6d71080cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n",
      "$ SYM\n"
     ]
    }
   ],
   "source": [
    "s=nlp('The cost of Iphone in U.K is 699$')\n",
    "for tokens in s:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "284359e6-e2c9-4f66-aefe-681d67562508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The' DET\n",
      "'cost' NOUN\n",
      "'of' ADP\n",
      "'Iphone' PROPN\n",
      "'in' ADP\n",
      "'U.K' PROPN\n",
      "'is' AUX\n",
      "'699' NUM\n",
      "'$' SYM\n"
     ]
    }
   ],
   "source": [
    "for token in s:\n",
    "    print(repr(token.text), token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e16ab970-a071-44df-ab9c-32ee5008f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first N.Y.C sentence.\n",
      "I gave given fullstop please check.\n",
      "Let's study now\n"
     ]
    }
   ],
   "source": [
    "s=nlp(u\"This is the first N.Y.C sentence. I gave given fullstop please check. Let's study now\")\n",
    "for sentence in s.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32a60e2-0530-493c-ade0-ac0847e277e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'show', 'could', 'she', 'yourself', 'that', 'part', 'your', 'be', 'therein', 'least', 'were', 'sometime', 'last', 'more', 'full', 'a', 'seemed', 'you', 'as', 'if', 'rather', 'side', 'bottom', 'due', 'for', 'go', 'all', 'call', 'whether', \"'re\", 'does', 'has', 'everywhere', 'mostly', 'again', 'between', 'alone', 'both', 'beside', 'where', 'four', 'have', \"'m\", '‘d', 'yours', 'amongst', 'i', 'her', 'hereby', 'something', 'also', 'thereafter', 'being', '’ll', 'anyone', \"'ll\", 'the', '’re', 'below', 'no', 'not', 'whereafter', 'most', 'until', 'me', 'never', 'made', 'give', 'say', 'although', 'anyhow', 'would', 'herein', 'much', 'into', 'while', '‘m', 'whole', 'further', 'though', 'nothing', 'with', 'are', 'indeed', 'else', 'however', 'meanwhile', 'noone', 'of', 'using', 'put', 'it', 'seems', 'everyone', 'since', \"'s\", 'ca', 'serious', 'former', 'whenever', 'twelve', 'along', 'done', 'from', 'nevertheless', 'sometimes', 'anywhere', 'off', \"'ve\", 'under', 'once', 'whereas', 'can', 'ever', 'keep', 'thereupon', 're', 'somehow', 'about', 'there', 'this', 'too', 'what', 'n’t', 'around', 'mine', 'sixty', 'just', 'wherein', 'see', 'six', '‘s', 'nor', 'either', 'moreover', 'by', 'often', 'should', 'some', 'beforehand', 'become', '’m', 'nine', 'which', 'hers', 'back', 'behind', 'afterwards', 'within', 'when', 'because', 'themselves', 'was', 'elsewhere', 'via', 'himself', 'above', 'wherever', 'whence', 'toward', 'any', 'many', 'became', 'except', 'here', 'cannot', 'and', 'is', 'ourselves', 'everything', '’d', 'unless', 'very', 'perhaps', 'doing', 'myself', 'had', 'whom', 'or', 'every', 'whereupon', 'my', 'their', 'been', '’s', 'beyond', 'against', 'others', 'even', 'somewhere', 'eight', 'then', 'thru', 'few', 'thereby', 'latter', 'do', 'us', 'itself', 'anyway', 'did', 'up', 'only', 'therefore', 'amount', 'anything', 'third', 'at', 'yourselves', 'almost', \"n't\", 'namely', 'front', 'always', 'hereupon', 'seeming', 'such', '‘ll', 'neither', 'next', 'hereafter', 'onto', 'during', 'two', 'through', 'five', 'he', \"'d\", 'our', 'still', 'per', 'three', 'those', 'upon', 'an', 'enough', 'him', 'each', 'top', 'move', 'less', 'formerly', 'none', 'ten', 'whither', 'must', 'becomes', 'eleven', 'nowhere', 'otherwise', 'name', 'am', 'nobody', 'get', 'hence', 'twenty', 'well', 'may', 'latterly', 'over', 'herself', 'forty', 'whoever', 'throughout', 'fifty', 'his', 'yet', 'really', 'than', 'will', 'becoming', 'down', 'used', 'together', 'hundred', 'take', 'to', 'before', 'quite', 'other', 'another', 'how', 'they', 'whereby', 'who', 'thence', 'towards', 'whose', 'we', 'after', 'so', 'various', 'regarding', 'please', '‘ve', 'its', 'own', 'in', 'among', 'without', 'now', 'someone', 'make', 'empty', 'fifteen', 'several', 'them', 'out', 'one', 'seem', 'first', 'across', 'same', '’ve', '‘re', 'already', 'whatever', 'might', 'but', 'why', 'on', 'these', 'ours', 'thus', 'n‘t', 'besides'}\n",
      "\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.Defaults.stop_words)\n",
    "print()\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "292e02f0-0e11-4c29-be9c-529a64343411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab['is'].is_stop)\n",
    "print(nlp.vocab['GFG'].is_stop)\n",
    "print(nlp.vocab['Luan'].is_stop)\n",
    "print(nlp.vocab['was'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c855bf-223e-4749-a8b7-7ad46b003b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab['i.e'].is_stop)\n",
    "nlp.vocab['i.e'].is_stop = True\n",
    "print(nlp.vocab['i.e'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e42815-1465-4625-ad15-0b48a1998d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab['done'].is_stop)\n",
    "nlp.Defaults.stop_words.remove('done')\n",
    "nlp.vocab['done'].is_stop = False\n",
    "print(nlp.vocab['done'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b59db38-5354-4e91-95f8-2703fc03b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='''Data science is the study of data. Like biological sciences is a study of biology, physical sciences, it's the study of physical reactions. Data is real, data has real properties, and we need to study them if we're going to work on them. Data Science involves data and some signs. It is a process, not an event. It is the process of using data to understand too many different things, to understand the world. Let Suppose when you have a model or proposed explanation of a problem, and you try to validate that proposed explanation or model with your data. It is the skill of unfolding the insights and trends that are hiding (or abstract) behind data. It's when you translate data into a story. So use storytelling to generate insight. And with these insights, you can make strategic choices for a company or an institution. We can also define data science as a field that is about processes and systems to extract data of various forms and from various resources whether the data is unstructured or structured.\n",
    "The definition and the name came up in the 1980s and 1990s when some professors, IT Professionals, scientists were looking into the statistics curriculum, and they thought it would be better to call it data science and then later on data analytics derived.\n",
    "'''\n",
    "txt=txt.replace('\\n','')\n",
    "txt=txt.replace('  ','')\n",
    "txt=txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a2abd4-4f82-4cca-a05e-12065fbbcc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'And', 'then', 'we', 'it', 'can', 'many', 'various', 'The', 'or', \"'s\", 'some', 'would', 'a', 'you', 'if', 'into', 'as', 'about', 'that', 'too', 'So', 'and', 'up', 'have', 'behind', 'in', 'for', 'not', 'is', 'when', 'name', 'with', 'on', 'make', 'from', 'We', 'also', 'your', 'be', 'are', 'to', 'them', 'call', 'an', 'whether', 'these', 'It', \"'re\", 'they', 'using', 'has', 'of', 'were', 'the', 'IT'}\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "txt = nlp(txt)\n",
    "stop_words=set()\n",
    "\n",
    "for token in txt:\n",
    "  if token.is_stop:\n",
    "    stop_words.add(token.text)\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e80b1c1-7fde-4fd3-9399-5908c4901e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science study data . Like biological sciences study biology , physical sciences , study physical reactions . Data real , data real properties , need study going work . Data Science involves data signs . process , event . process data understand different things , understand world . Let Suppose model proposed explanation problem , try validate proposed explanation model data . skill unfolding insights trends hiding ( abstract ) data . translate data story . use storytelling generate insight . insights , strategic choices company institution . define data science field processes systems extract data forms resources data unstructured structured . definition came 1980s 1990s professors , Professionals , scientists looking statistics curriculum , thought better data science later data analytics derived .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([token.text for token in txt if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c59aea-c10b-44be-9143-7bd2dcd8243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4504f1e5-c703-4af9-93fe-2154b4615d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a carnival performer who does disgusting acts\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets('Geek')\n",
    "print(syn[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e6a2ed7-ceaf-4ad1-9d0a-9ff48d397371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: book.n.01\n",
      "Definition: a written work or composition that has been published (printed on pages bound together)\n",
      "Examples: ['I am reading a good book on economics']\n",
      "\n",
      "Synset: book.n.02\n",
      "Definition: physical objects consisting of a number of pages bound together\n",
      "Examples: ['he used a large book as a doorstop']\n",
      "\n",
      "Synset: record.n.05\n",
      "Definition: a compilation of the known facts regarding something or someone\n",
      "Examples: [\"Al Smith used to say, `Let's look at the record'\", 'his name is in all the record books']\n",
      "\n",
      "Synset: script.n.01\n",
      "Definition: a written version of a play or other dramatic composition; used in preparing for a performance\n",
      "Examples: []\n",
      "\n",
      "Synset: ledger.n.01\n",
      "Definition: a record in which commercial accounts are recorded\n",
      "Examples: ['they got a subpoena to examine our books']\n",
      "\n",
      "Synset: book.n.06\n",
      "Definition: a collection of playing cards satisfying the rules of a card game\n",
      "Examples: []\n",
      "\n",
      "Synset: book.n.07\n",
      "Definition: a collection of rules or prescribed standards on the basis of which decisions are made\n",
      "Examples: ['they run things by the book around here']\n",
      "\n",
      "Synset: koran.n.01\n",
      "Definition: the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina\n",
      "Examples: []\n",
      "\n",
      "Synset: bible.n.01\n",
      "Definition: the sacred writings of the Christian religions\n",
      "Examples: ['he went to carry the Word to the heathen']\n",
      "\n",
      "Synset: book.n.10\n",
      "Definition: a major division of a long written composition\n",
      "Examples: ['the book of Isaiah']\n",
      "\n",
      "Synset: book.n.11\n",
      "Definition: a number of sheets (ticket or stamps etc.) bound together on one edge\n",
      "Examples: ['he bought a book of stamps']\n",
      "\n",
      "Synset: book.v.01\n",
      "Definition: engage for a performance\n",
      "Examples: ['Her agent had booked her for several concerts in Tokyo']\n",
      "\n",
      "Synset: reserve.v.04\n",
      "Definition: arrange for and reserve (something for someone else) in advance\n",
      "Examples: ['reserve me a seat on a flight', 'The agent booked tickets to the show for the whole family', \"please hold a table at Maxim's\"]\n",
      "\n",
      "Synset: book.v.03\n",
      "Definition: record a charge in a police register\n",
      "Examples: ['The policeman booked her when she tried to solicit a man']\n",
      "\n",
      "Synset: book.v.04\n",
      "Definition: register in a hotel booker\n",
      "Examples: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets('Book'):\n",
    "    print(\"Synset:\", syn.name())\n",
    "    print(\"Definition:\", syn.definition())\n",
    "    print(\"Examples:\", syn.examples())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fa4d363-7f18-45b2-aab4-0509523a3e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for s in wordnet.synsets('happy'):\n",
    "    for lemma in s.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d7bf0e-cca6-425b-957a-687c836a7db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'goodness', 'good']\n"
     ]
    }
   ],
   "source": [
    "antonyms = []\n",
    "for s in wordnet.synsets('evil'):\n",
    "    for lemma in s.lemmas():\n",
    "        for ant_lemma in lemma.antonyms():\n",
    "            antonyms.append(ant_lemma.name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31f9882a-7ee4-4cce-9405-cb70c938ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India, officially the Republic of India Hindi: Bhārat Gaṇarājya, is a country in South Asia...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = '''India, officially the Republic of India (Hindi: Bhārat Gaṇarājya),[25] is a country in South Asia...''' \n",
    "corpus = corpus.replace(\"[25]\",\"\").replace(\"[f]\",\"\").replace(\")\",\"\").replace('(',\"\")\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59bc1cd6-f949-4c6f-a2db-f9cea0e18f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['india', 'officially', 'republic', 'india', 'hindi', 'bhārat', 'gaṇarājya', 'country', 'south', 'asia', '...']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for word in word_tokenize(corpus):\n",
    "    if word.lower() not in stop_words and len(word) >= 2:\n",
    "        words.append(word.lower())\n",
    "\n",
    "print(\"Filtered Words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54237783-a2b1-4ecd-a1d7-fb9d9ae84a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10\n",
      "Sample Vocabulary: ['india', 'hindi', 'gaṇarājya', 'bhārat', 'asia', 'republic', '...', 'south', 'country', 'officially']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(words))\n",
    "print(\"Vocabulary size:\",len(vocab))\n",
    "print(\"Sample Vocabulary:\",vocab[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f028b7bb-91e9-42c6-a258-8c8c6364a557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-to-Number: 1\n",
      "Number-to-word: hindi\n"
     ]
    }
   ],
   "source": [
    "num=1\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "\n",
    "for word in vocab:\n",
    "    word_to_num[word] = num\n",
    "    num_to_word[num] = word\n",
    "    num+=1\n",
    "print(\"Word-to-Number:\",word_to_num['india'])\n",
    "print(\"Number-to-word:\", num_to_word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b1ca5c2-1352-4107-a052-06dc29ac9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = '''India, officially the Republic of India (Hindi: Bhārat Gaṇarājya),[25] is a country in South Asia...'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22d17671-c731-45a8-ae49-8ec9ff82d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.replace(\"(\",'').replace(')','').replace('[25]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a729f18-e0ef-443c-b074-be2df0827cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'officially', 'republic', 'india', 'hindi', 'bhārat', 'gaṇarājya', 'country', 'south', 'asia', '...']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for word in word_tokenize(corpus):\n",
    "    if word not in stop_words and len(word) >= 2:\n",
    "        words.append(word.lower())\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "be99fd25-cf77-4517-ab45-0dce6138baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'hindi', 'gaṇarājya', 'bhārat', 'asia', 'republic', '...', 'south', 'country', 'officially']\n",
      "Word to num: 1\n",
      "Num to word: hindi\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(words))\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "num = 1\n",
    "print(vocab)\n",
    "\n",
    "for word in vocab:\n",
    "    word_to_num[word.lower()]=num\n",
    "    num_to_word[num]= word\n",
    "    num+=1\n",
    "\n",
    "print(\"Word to num:\",word_to_num['India'.lower()])\n",
    "print(\"Num to word:\",num_to_word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1dd739bb-64d8-42df-9fa5-0821e2990179",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''India, officially the Republic of India (Hindi: Bhārat Gaṇarājya, is a country in South Asia.\n",
    "It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.\n",
    "Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.\n",
    "In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6e82164-1762-4192-bbf9-b7690732c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for word in word_tokenize(corpus):\n",
    "    if (word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
    "        words.append(word.lower())\n",
    "\n",
    "vocab=list(set(words))\n",
    "len(vocab)\n",
    "\n",
    "num=1\n",
    "word_to_num={}\n",
    "num_to_word={}\n",
    "for word in vocab:\n",
    "    word_to_num[word]=num\n",
    "    num_to_word[num]=word\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "240d53ef-80d5-4b1b-9899-4f6b56cf1ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India 1 officially 5 Republic 8 India 1 Hindi 38 Bhārat 17 Gaṇarājya 16 country 47 South 12 Asia 18 \n",
      "seventh-largest 22 country 47 area 42 second-most 15 populous 48 country 47 populous 48 democracy 43 world 4 \n",
      "Bounded 37 Indian 23 Ocean 21 south 12 Arabian 40 Sea 36 southwest 13 Bay 9 Bengal 20 southeast 39 shares 28 land 32 borders 33 Pakistan 6 west 31 China 46 Nepal 7 Bhutan 14 north 41 Bangladesh 24 Myanmar 10 east 26 \n",
      "Indian 23 Ocean 21 India 1 vicinity 44 Sri 2 Lanka 25 Maldives 19 Andaman 30 Nicobar 29 Islands 3 share 45 maritime 11 border 34 Thailand 27 Myanmar 10 Indonesia 35 \n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(corpus):\n",
    "    for word in word_tokenize(sent):\n",
    "        if (word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
    "            print(word,end=' ')\n",
    "            print(word_to_num[word.lower()],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc794acd-b61d-430a-92b9-11dbb862ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India 1 officially 5 Republic 8 India 1 Hindi 38 Bhārat 17 Gaṇarājya 16 country 47 South 12 Asia 18 seventh-largest 22 country 47 area 42 second-most 15 populous 48 country 47 populous 48 democracy 43 world 4 Bounded 37 Indian 23 Ocean 21 south 12 Arabian 40 Sea 36 southwest 13 Bay 9 Bengal 20 southeast 39 shares 28 land 32 borders 33 Pakistan 6 west 31 China 46 Nepal 7 Bhutan 14 north 41 Bangladesh 24 Myanmar 10 east 26 Indian 23 Ocean 21 India 1 vicinity 44 Sri 2 Lanka 25 Maldives 19 Andaman 30 Nicobar 29 Islands 3 share 45 maritime 11 border 34 Thailand 27 Myanmar 10 Indonesia 35 \n"
     ]
    }
   ],
   "source": [
    "for word in word_tokenize(corpus):\n",
    "        if (word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
    "            print(word,end=' ')\n",
    "            print(word_to_num[word.lower()],end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "09036ad6-861e-4add-a371-cd2b880e02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 8, 1, 38, 17, 16, 47, 12, 18]\n",
      "\n",
      "[22, 47, 42, 15, 48, 47, 48, 43, 4]\n",
      "\n",
      "[37, 23, 21, 12, 40, 36, 13, 9, 20, 39, 28, 32, 33, 6, 31, 46, 7, 14, 41, 24, 10, 26]\n",
      "\n",
      "[23, 21, 1, 44, 2, 25, 19, 30, 29, 3, 45, 11, 34, 27, 10, 35]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for sent in sent_tokenize(corpus):\n",
    "    temp=[]\n",
    "    for word in word_tokenize(sent):\n",
    "        if (word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
    "            #print(word,end=' ')\n",
    "            temp.append(word_to_num[word.lower()])\n",
    "    print(temp)\n",
    "    data.append(temp)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca36bc5e-2cd2-46a1-b27a-f749ead2fdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india officially republic india hindi bhārat gaṇarājya country south asia \n",
      "seventh-largest country area second-most populous country populous democracy world \n",
      "bounded indian ocean south arabian sea southwest bay bengal southeast shares land borders pakistan west china nepal bhutan north bangladesh myanmar east \n",
      "indian ocean india vicinity sri lanka maldives andaman nicobar islands share maritime border thailand myanmar indonesia \n"
     ]
    }
   ],
   "source": [
    "for sent in data:\n",
    "    for word in sent:\n",
    "        print(num_to_word[word],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60349558-3c49-4b9f-95f3-487e2fca4bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16af0897-8fff-43d5-a155-8e1beaae4827",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
